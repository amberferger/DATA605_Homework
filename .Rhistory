rlang::last_error()
testing %>% filter(is.na(testing))
testing[is.na(testing)] <- 0
is.na(testing
)
is.na(testing) == TRUE
testing[1556,]
View(testSet$Id==1556)
View(testSet %>% filter(Id==1556))
View(testing)
nrow(testing)
nrow(testSet)
View(testSet)
testing <- testSet
testing$RemodInd <- with(testing, ifelse(YearRemodAdd==YearBuilt, 0,1))
testing$PropFront <- with(testing, LotFrontage/LotArea)
testing$PropBsmtFin <- with(testing, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
testing$funcTyp <- with(testing, ifelse(Functional == 'Typ', 1,0))
testing$RR <- with(testing, ifelse(Condition1 == 'Norm', 1, 0))
testing$nbrhd <- with(testing, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
testing <- testing %>%
dplyr::select(Id, GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, KitchenQual,
X2ndFlrSF, RR, nbrhd)
View(testing %>%
filter(Id==1556))
View(test %>% select(KitchenQual) %>% distinct())
View(testSet %>% select(KitchenQual) %>% distinct())
View(testSet %>% select(KitchenQual) %>% distinct())
View(testSet %>% dplyr::select(KitchenQual) %>% distinct())
View(testSet %>%
dplyr::group_by(KitchenQual) %>%
summarize(CON = n())
)
---
title: "DATA 605 - Final Exam"
author: "Amber Ferger"
date: "5/15/2020"
output: html_document
---
```{r, include=FALSE}
library(tidyverse)
library(reshape)
library(matlib)
library(matrixcalc)
library(MASS)
```
## Problem 1
**Generate a random variable $X$ that has $10,000$ random uniform numbers from $1$ to $N$, where $N$ can be any number of your choosing greater than or equal to $6$.  Then generate a random variable $Y$ that has $10,000$ random normal numbers with a mean of $\mu = \sigma = \frac{N+1}{2}$.**
```{r}
set.seed(124)
n <- 10000
N <- 10
mu <- (N+1)/2
sd <- (N+1)/2
X <- runif(n, min = 1, max = N)
Y <- rnorm(n, mean= mu, sd = sd)
```
### Probability (5 pts)
**Calculate as a minimum the below probabilities $a$ through $c$.  Assume the small letter "x" is estimated as the median of the $X$ variable, and the small letter "y" is estimated as the 1st quartile of the $Y$ variable.  Interpret the meaning of all probabilities.**
```{r}
x <- median(X)
y <- as.numeric(quantile(Y)[2])
z <- (y - mu) / sd
pXlx <- ((x-min(X))*(1/N))
pXgx <- 1 - ((x-min(X))*(1/N))
pXgy <- 1 - ((y-min(X)) * (1/N))
pYgy <- 1 - pnorm(z)
pYly <- pnorm(z)
```
**a. P(X>x | X>y)**
Conditional probability - probability that $X$ is greater than $x$ given that $X$ is greater than $y$.
$$P(X>x | X>y) = \frac{P(X>x \cap X>y)}{P(X>y)} = \frac{P(X>x) P(X>y)}{P(X>y)}$$
```{r}
(pXgx*pXgy)/pXgy
```
**b. P(X>x, Y>y)**
Joint Probability - Probability that $X$ is greater than $x$ and $Y$ is greater than $y$.
$$P(X>x, Y>y) = P(X>x \cap Y>y) = P(X>x) P(Y>y)$$
```{r}
pXgx*pYgy
```
**c. P(X<x | X>y)**
Conditional probability - probability that $X$ is less than $x$ given that $X$ is greater than $y$.
$$P(X<x | X>y) = \frac{P(X<x \cap X>y)}{P(X>y)} = \frac{P(X<x) P(X>y)}{P(X>y)}$$
```{r}
(pXlx*pXgy)/pXgy
```
### Probability (5 pts)
**Investigate whether $P(X>x$ $and$ $Y>y) = P(X>x)P(Y>y)$ by building a table and evaluating the marginal and joint probabilities.**
We will create a matrix of joint probabilities and then calculate the marginal probabilities. We will then compare the joint probability corresponding to record $[1,1]$ in the matrix to the product of the marginal probabilities (records $[3,1]$ and $[1,3]$ in the matrix).
```{r}
# calculate joint probabilities
a <- pXgx * pYgy
b <- pXgx * pYly
c <- pXlx * pYgy
d <- pXlx * pYly
# create matrix
jointMatrix <- matrix(c(a,b,c,d),
nrow=2)
# add row totals
fullMatrix <- rbind(jointMatrix, c(a+b, c+d))
# add column totals
fullMatrix <- cbind(fullMatrix, c(a+c, b+d, NA))
colnames(fullMatrix) <- c('P(X>x)', 'P(X<x)', 'Marginal Prob Y')
rownames(fullMatrix) <- c('P(Y>y)', 'P(Y<y)', 'Marginal Prob X')
fullMatrix
```
```{r}
margProb <- fullMatrix[3,1] * fullMatrix[1,3]
jointProb <- fullMatrix[1,1]
margProb
jointProb
```
The two probabilities are equal.
### Probability (5 pts)
**Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?**
Random variables are independent if neither variable affects the probability distribution of the other. Both Fisher's and Chi-Square examine the likelihood of independence of given variables.
In both cases, we test the hypothesis that the variables are independent:
* $H_0$: The two variables are independent.
* $H_A$: The two variables are not independent.
A $p-value \leq 0.05$ allows us to reject $H_0$ and accept $H_A$. In this case, we can prove that the two variables are independent if the $p-value > 0.05$.
```{r warning=FALSE}
fisher.result <- fisher.test(jointMatrix)
print(fisher.result$p.value)
```
```{r warning=FALSE}
chi.result <- chisq.test(jointMatrix)
print(chi.result$p.value)
```
In both instances, the p-value is very high $(1)$, which means we accept $H_0$ - the variables are independent.
The chi-square test can be used for any sized contingency table, whereas Fisher's exact test can only be used for $2x2$ contingency tables. Additionally, from reading online, it looks like Fisher's exact test is generally preferred for small sample sizes, whereas chi-square may be unreliable if the sample is too small.
In this case, I would recommend using the chi-square test because we have 2 random variables that both have $10,000$ numbers, to this test might be more accurate.
# Problem 2
**You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.  https://www.kaggle.com/c/house-prices-advanced-regression-techniques **
```{r}
trainSet <- read.csv('train.csv', sep=',') %>%
as_tibble()
testSet <- read.csv('test.csv', sep=',') %>%
as_tibble()
```
## Descriptive and Inferential Statistics (5 points)
**Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any three quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?**
### Plot the data
We'll take a look at the sale prices:
```{r}
hist(trainSet$SalePrice,
main="Distribution of Sale Prices",
xlab="Sale Price",
breaks = 20)
```
The majority of homes fall between $\$100,000$ and $\$200,000$.
``` {r}
boxplot(trainSet$SalePrice,
main="Boxplot of Sale Prices",
xlab="Sale Price")
```
The boxplot shows us that we have quite a few outliers.
### Univariate Descriptive Statistics
```{r}
summary(trainSet)
```
### Scatterplot Matrix
* **Dependent Variable**: SalePrice
* **Independent Variables**: OverallQual, OverallCond
```{r}
# trainset[c('SalePrice', 'OverallQual', 'OverallCond')]
spMatrix <- trainSet %>%
dplyr::select(SalePrice, OverallQual, OverallCond)
pairs(spMatrix, gap=0.2)
```
**Quality** - We can see from the graph below that as the quality of the home increases, so does the sale price.
**Condition** - There's a lot more variation in the sale price compared to the overall condition of the home (especially around condition = 5), but in general, we can see as the condition improves, the sale price increases.
### Correlation Matrix
* **Quantitative Variables**: LotArea, TotalBsmtSF, GrLivArea
```{r}
matrixVals <- trainSet %>%
dplyr::select(LotArea, TotalBsmtSF, GrLivArea)
corrMatrix <- cor(matrixVals)
corrMatrix
```
None of the variables appear to be that correlated -- the highest correlation is between GrLivArea and TotalBsmtSF.
### Pairwise correlations
For this analysis, we will assume the following:
* $H_0$: The true correlation between the variables is $0$.
* $H_A$: The true correlation between the variables is not $0$.
If the $p-value \leq 0.05$, we will reject $H_0$ and accept $H_A$. This would mean that there is a non-zero correlation between the variables.
**Lot Area vs Total Basement Square footage**
```{r}
laBsf <- cor.test(matrixVals$LotArea, matrixVals$TotalBsmtSF, method = 'pearson',conf.level = 0.8)
laBsf$p.value
```
**Lot Area vs Living Area**
```{r}
laLa <- cor.test(matrixVals$LotArea, matrixVals$GrLivArea, method = 'pearson',conf.level = 0.8)
laLa$p.value
```
**Total Basement Square footage vs Living Area**
```{r}
BsfLa <- cor.test(matrixVals$TotalBsmtSF, matrixVals$GrLivArea, method = 'pearson',conf.level = 0.8)
BsfLa$p.value
```
In each pairwise-comparison, the $p-value <= 0.05$, which means that we can reject $H_0$ and accept the null. This means that the variables are not independent of each other - and regardless of how slight, they are all at least somewhat correlated to each other.
Familywise error is the likelihood of accepting a false positive - in this case, reporting that there is a correlation between variables when in fact they have occurred by chance. Because the $p-values$ are so much lower than $0.05$, I would not be worried about familywise error.
### Linear Algebra and Correlation (5 points)
**Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix. **
```{r}
precMatrix  <- inv(corrMatrix)
precMatrix
```
```{r}
cp <- corrMatrix %*% precMatrix
pc <- precMatrix %*% corrMatrix
cp
pc
```
The precision matrix * correlation matrix is about equal to the correlation matrix * precision matrix, which makes sense because they are inverses.
LU Decomposition - I am shortcutting for this problem by using the lu.decomposition function from the matrixcalc library. To see a more detailed analysis of the LU decomposition function, please visit: https://rpubs.com/amberferger/DATA605_HW2_PS2
```{r}
lu <- lu.decomposition(corrMatrix)
l <- lu$L
u <- lu$U
l
u
```
We can check that the LU Decomposition worked by comparing $l * u$ to the original matrix:
```{r}
l %*% u
corrMatrix
```
### Calculus-Based Probability & Statistics (5 points)
**Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.  Then load the MASS package and run fitdistr to fit an exponential probability density function.  (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of $\lambda$ for this distribution, and then take $1,000$ samples from this exponential distribution using this value (e.g., $rexp(1,000, \lambda)$). Plot a histogram and compare it with a histogram of your original variable. Using the exponential pdf, find the $5^{th}$ and $95^{th}$ percentiles using the cumulative distribution function (CDF). Also generate a $95\%$ confidence interval from the empirical data, assuming normality. Finally, provide the empirical $5^{th}$ percentile and $95^{th}$ percentile of the data. Discuss.**
I will use the variable **X1stFlrSF**.
```{r}
hist(trainSet$X1stFlrSF)
```
**Documentation for function output:**
An object of class "fitdistr", a list with four components,
* estimate - the parameter estimates
* sd - the estimated standard errors
* vcov - the estimated variance-covariance matrix
* loglik - the log-likelihood.
```{r}
exPDF <- fitdistr(trainSet$X1stFlrSF, "exponential")
lam <- as.numeric(exPDF$estimate)
samps <- rexp(1000,lam)
hist(samps,
main="Distribution of X1stFlrSF - Exponential")
```
```{r}
hist(trainSet$X1stFlrSF,
main="Distribution of X1stFlrSF")
```
### Modeling (10 points)
**Build some type of multiple regression  model and submit your model to the competition board. Provide your complete model summary and results with analysis. Report your Kaggle.com user name and score.**
After a number of trials (and errors), I've come up with the following set of features to use:
**From dataset:**
* GrLivArea
* OverallQual
* YearBuilt
* LotArea
* GarageCars
* KitchenQual
* 2ndFlrSF
**Engineered features:**
* RemodInd - indicator if the house has been remodeled
* PropFront - proportion of the yard that is frontage
* PropBsmtFin - proportion of the basement that is finished
* funcTyp - 1 if Functional = "Typ"
* RR - 1 if the Condition1 = "Norm"
* nbrhd - 1 if the house is in a set of neighborhoods
```{r}
multReg <- trainSet
multReg$RemodInd <- with(multReg, ifelse(YearRemodAdd==YearBuilt, 0,1))
multReg$PropFront <- with(multReg, LotFrontage/LotArea)
multReg$PropBsmtFin <- with(multReg, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
multReg$funcTyp <- with(multReg, ifelse(Functional == 'Typ', 1,0))
multReg$RR <- with(multReg, ifelse(Condition1 == 'Norm', 1, 0))
multReg$nbrhd <- with(multReg, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
multReg$kitchen <- with(multReg, ifelse(KitchenQual=='Ex' | KitchenQual=='Gd',1,0))
multReg <- multReg %>%
dplyr::select(SalePrice,GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, kitchen,
X2ndFlrSF, RR, nbrhd)
# we will replace all nulls with 0
multReg[is.na(multReg)] <- 0
```
```{r}
multReg.lm <- lm(SalePrice ~ GrLivArea  + OverallQual + YearBuilt + RemodInd + LotArea + PropFront + PropBsmtFin  + GarageCars  + funcTyp + KitchenQual + X2ndFlrSF + RR + nbrhd , data = multReg)
summary(multReg.lm)
```
**Analysis:**
* **F-statistic:* The F-test compares the current model to a model with one fewer predictor. If the current model is better than the reduced model, the p-value will be small. In this case, the p-value associated with the f-test is very small $(< 2.2e-16)$, so we can conclude that this model fits the data better than one with one fewer predictor.
* **Adjusted** $R^2$: The value of $0.8302$ means this set of features accounts for ~$83\%$ of the variance in Sale Price.
* **Standard Error**: This measures the variation in the residuals. As a general rule, this value should be 1.5 times the quartile values. For the first quartile: $13,780∗1.5=20,670$ and for the third quartile: $13,010∗1.5=19,515$. It is about 37% higher than the first quartile and 40% higher than the third quartile.These values are high, so we should expect that our residual plots will not be normal.
* **P-Values**: This measures the probability that the factor is not important in the model. The smaller the value, the more significant the factor is. All of the p-values are well below the threshold of $0.05$, so we know that all variables are significant.
We will now plot the residuals:
```{r}
plot(fitted(multReg.lm),resid(multReg.lm))
```
A good model will have a residual plot where (1) there’s no clear pattern and (2) the points hover both above and below 0. The curved pattern in the plot of our residuals indicates that we don’t have a great model. This indicates that we do not do as well predicting on higher-priced homes.
Additionally, in a good model we expect the residuals to be normally distributed. By graphing the data in a Q-Q plot, we can see how well the observations follow the line. If they fit the line well, we know that the data is normally distributed.
```{r}
qqnorm(resid(multReg.lm))
qqline(resid(multReg.lm))
```
There is clear deviation from the line in both ends of the plot, so this further shows that we do poorly at predicting the high and low priced houses.
Finally, we will make predictions on our test data:
```{r}
testing <- testSet
testing$RemodInd <- with(testing, ifelse(YearRemodAdd==YearBuilt, 0,1))
testing$PropFront <- with(testing, LotFrontage/LotArea)
testing$PropBsmtFin <- with(testing, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
testing$funcTyp <- with(testing, ifelse(Functional == 'Typ', 1,0))
testing$RR <- with(testing, ifelse(Condition1 == 'Norm', 1, 0))
testing$nbrhd <- with(testing, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
testing <- testing %>%
dplyr::select(Id, GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, KitchenQual,
X2ndFlrSF, RR, nbrhd)
View(testing %>%
filter(Id==1556))
View(testSet %>%
dplyr::group_by(KitchenQual) %>%
summarize(CON = n())
)
testing[is.na(testing)] <- 0
testing[is.na(testing)] <- ''
pred <- predict(multReg.lm, newdata=testing)
finalPred <- pred %>%
as_tibble
finalPred$Id <- testSet$Id
finalPred <- finalPred %>%
dplyr::select(Id,
SalePrice = value)
write.csv(finalPred,file='finalSubmission.csv', row.names=FALSE)
```
multReg <- trainSet
multReg$RemodInd <- with(multReg, ifelse(YearRemodAdd==YearBuilt, 0,1))
multReg$PropFront <- with(multReg, LotFrontage/LotArea)
multReg$PropBsmtFin <- with(multReg, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
multReg$funcTyp <- with(multReg, ifelse(Functional == 'Typ', 1,0))
multReg$RR <- with(multReg, ifelse(Condition1 == 'Norm', 1, 0))
multReg$nbrhd <- with(multReg, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
multReg$kitchen <- with(multReg, ifelse(KitchenQual=='Ex' | KitchenQual=='Gd',1,0))
multReg <- multReg %>%
dplyr::select(SalePrice,GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, kitchen,
X2ndFlrSF, RR, nbrhd)
# we will replace all nulls with 0
multReg[is.na(multReg)] <- 0
multReg.lm <- lm(SalePrice ~ GrLivArea  + OverallQual + YearBuilt + RemodInd + LotArea + PropFront + PropBsmtFin  + GarageCars  + funcTyp + kitchen + X2ndFlrSF + RR + nbrhd , data = multReg)
summary(multReg.lm)
multReg <- trainSet
multReg$RemodInd <- with(multReg, ifelse(YearRemodAdd==YearBuilt, 0,1))
multReg$PropFront <- with(multReg, LotFrontage/LotArea)
multReg$PropBsmtFin <- with(multReg, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
multReg$funcTyp <- with(multReg, ifelse(Functional == 'Typ', 1,0))
multReg$RR <- with(multReg, ifelse(Condition1 == 'Norm', 1, 0))
multReg$nbrhd <- with(multReg, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
multReg$kitchen <- with(multReg, ifelse(is.null(KitchenQual) | is.na(KitchenQual),'',KitchenQual))
multReg <- multReg %>%
dplyr::select(SalePrice,GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, kitchen,
X2ndFlrSF, RR, nbrhd)
# we will replace all nulls with 0
multReg[is.na(multReg)] <- 0
multReg.lm <- lm(SalePrice ~ GrLivArea  + OverallQual + YearBuilt + RemodInd + LotArea + PropFront + PropBsmtFin  + GarageCars  + funcTyp + kitchen + X2ndFlrSF + RR + nbrhd , data = multReg)
summary(multReg.lm)
multReg$KitchenQual <- with(multReg, ifelse(is.null(KitchenQual) | is.na(KitchenQual),'',KitchenQual))
multReg <- trainSet
multReg$RemodInd <- with(multReg, ifelse(YearRemodAdd==YearBuilt, 0,1))
multReg$PropFront <- with(multReg, LotFrontage/LotArea)
multReg$PropBsmtFin <- with(multReg, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
multReg$funcTyp <- with(multReg, ifelse(Functional == 'Typ', 1,0))
multReg$RR <- with(multReg, ifelse(Condition1 == 'Norm', 1, 0))
multReg$nbrhd <- with(multReg, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
multReg$KitchenQual <- with(multReg, ifelse(is.null(KitchenQual) | is.na(KitchenQual),'',KitchenQual))
multReg <- multReg %>%
dplyr::select(SalePrice,GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, KitchenQual,
X2ndFlrSF, RR, nbrhd)
# we will replace all nulls with 0
multReg[is.na(multReg)] <- 0
multReg.lm <- lm(SalePrice ~ GrLivArea  + OverallQual + YearBuilt + RemodInd + LotArea + PropFront + PropBsmtFin  + GarageCars  + funcTyp + KitchenQual + X2ndFlrSF + RR + nbrhd , data = multReg)
summary(multReg.lm)
multReg.lm <- lm(SalePrice ~ GrLivArea  + OverallQual + YearBuilt + RemodInd + LotArea + PropFront + PropBsmtFin  + GarageCars  + funcTyp  + X2ndFlrSF + RR + nbrhd , data = multReg)
summary(multReg.lm)
multReg.lm <- lm(SalePrice ~ GrLivArea  + OverallQual + YearBuilt + RemodInd + LotArea + PropFront + PropBsmtFin  + GarageCars  + funcTyp + KitchenQual + X2ndFlrSF + RR + nbrhd , data = multReg)
summary(multReg.lm)
plot(fitted(multReg.lm),resid(multReg.lm))
qqnorm(resid(multReg.lm))
qqline(resid(multReg.lm))
testing <- testSet
testing$RemodInd <- with(testing, ifelse(YearRemodAdd==YearBuilt, 0,1))
testing$PropFront <- with(testing, LotFrontage/LotArea)
testing$PropBsmtFin <- with(testing, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
testing$funcTyp <- with(testing, ifelse(Functional == 'Typ', 1,0))
testing$RR <- with(testing, ifelse(Condition1 == 'Norm', 1, 0))
testing$nbrhd <- with(testing, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
testing$KitchenQual <- with(testing, ifelse(is.null(KitchenQual) | is.na(KitchenQual),'',KitchenQual))
testing <- testing %>%
dplyr::select(Id, GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, KitchenQual,
X2ndFlrSF, RR, nbrhd)
testing[is.na(testing)] <- 0
pred <- predict(multReg.lm, newdata=testing)
str(testing$KitchenQual)
str(multRegKitchenQual)
str(multReg$KitchenQual)
testing <- testSet
testing$RemodInd <- with(testing, ifelse(YearRemodAdd==YearBuilt, 0,1))
testing$PropFront <- with(testing, LotFrontage/LotArea)
testing$PropBsmtFin <- with(testing, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
testing$funcTyp <- with(testing, ifelse(Functional == 'Typ', 1,0))
testing$RR <- with(testing, ifelse(Condition1 == 'Norm', 1, 0))
testing$nbrhd <- with(testing, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
testing$KitchenQual <- with(testing, ifelse(is.null(KitchenQual) | is.na(KitchenQual),'',KitchenQual))
testing$KitchenQual <- as.integer(testing$KitchenQual)
testing <- testing %>%
dplyr::select(Id, GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, KitchenQual,
X2ndFlrSF, RR, nbrhd)
testing[is.na(testing)] <- 0
pred <- predict(multReg.lm, newdata=testing)
finalPred <- pred %>%
as_tibble
finalPred$Id <- testSet$Id
finalPred <- finalPred %>%
dplyr::select(Id,
SalePrice = value)
write.csv(finalPred,file='finalSubmission.csv', row.names=FALSE)
testing <- testSet
testing$RemodInd <- with(testing, ifelse(YearRemodAdd==YearBuilt, 0,1))
testing$PropFront <- with(testing, LotFrontage/LotArea)
testing$PropBsmtFin <- with(testing, (TotalBsmtSF - BsmtUnfSF)/TotalBsmtSF)
testing$funcTyp <- with(testing, ifelse(Functional == 'Typ', 1,0))
testing$RR <- with(testing, ifelse(Condition1 == 'Norm', 1, 0))
testing$nbrhd <- with(testing, ifelse(Neighborhood == 'StoneBr' | Neighborhood == 'NridgHt' | Neighborhood == 'NoRidge' | Neighborhood == 'Somerst' | Neighborhood == 'Crawfor',1,0))
testing$KitchenQual <- with(testing, ifelse(is.null(KitchenQual) | is.na(KitchenQual),'',KitchenQual))
testing$KitchenQual <- as.integer(testing$KitchenQual)
testing <- testing %>%
dplyr::select(Id, GrLivArea, OverallQual, YearBuilt,
RemodInd, LotArea, PropFront, PropBsmtFin,
GarageCars, funcTyp, KitchenQual,
X2ndFlrSF, RR, nbrhd)
testing[is.na(testing)] <- 0
pred <- predict(multReg.lm, newdata=testing)
finalPred <- pred %>%
as_tibble
finalPred$Id <- testSet$Id
finalPred <- finalPred %>%
dplyr::select(Id,
SalePrice = value)
write.csv(finalPred,file='finalSubmission.csv', row.names=FALSE)
qexp(c(.05,0.95), samps=fit$estimate)
qexp(c(.05,0.95), rate=samps$estimate)
samps$estimate
qexp(c(.05,0.95), rate=exPDF$estimate)
quantile(trainSet$X1stFlrSF, c(0.05,0.95))
pnorm(.95)
qnorm(0.95)
n <- nrow(trainSet$X1stFlrSF)
avg <- mean(trainSet$X1stFlrSF)
sd <- sd(trainSet$X1stFlrSF)
ser <- pnorm(0.95)*sd/sqrt(n)
n
trainSet$X1stFlrSF
nrow(trainSet$X1stFlrSF)
str(trainSet)
nrow(trainSet)
n <- nrow(trainSet)
avg <- mean(trainSet$X1stFlrSF)
sd <- sd(trainSet$X1stFlrSF)
ser <- pnorm(0.95)*sd/sqrt(n)
lower <- avg - ser
upper <- avg + ser
lower
upper
quantile(trainSet$X1stFlrSF, c(0.05,0.95))
n <- nrow(trainSet)
avg <- mean(trainSet$X1stFlrSF)
sdev <- sd(trainSet$X1stFlrSF)
ser <- (pnorm(0.95)*sdev)/sqrt(n)
lower <- avg - ser
upper <- avg + ser
lower
upper
n <- nrow(trainSet)
avg <- mean(trainSet$X1stFlrSF)
sdev <- sd(trainSet$X1stFlrSF)
ser <- (qnorm(0.95)*sdev)/sqrt(n)
lower <- avg - ser
upper <- avg + ser
lower
upper
n <- nrow(trainSet)
avg <- mean(trainSet$X1stFlrSF)
sdev <- sd(trainSet$X1stFlrSF)
ser <- (pnorm(0.95)*sdev)/sqrt(n)
lower <- avg - ser
upper <- avg + ser
lower
upper
