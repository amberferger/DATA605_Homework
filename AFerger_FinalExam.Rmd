---
title: "DATA 605 - Final Exam"
author: "Amber Ferger"
date: "5/15/2020"
output: html_document
---

```{r, include=FALSE}

library(tidyverse)
library(dplyr)
library(reshape)
library(matlib)
library(matrixcalc)
library(MASS)

```

## Problem 1
**Generate a random variable $X$ that has $10,000$ random uniform numbers from $1$ to $N$, where $N$ can be any number of your choosing greater than or equal to $6$.  Then generate a random variable $Y$ that has $10,000$ random normal numbers with a mean of $\mu = \sigma = \frac{N+1}{2}$.**  

```{r}
set.seed(124)
n <- 10000
N <- 10
mu <- (N+1)/2
sd <- (N+1)/2
X <- runif(n, min = 1, max = N)
Y <- rnorm(n, mean= mu, sd = sd)

```


### Probability (5 pts)  

**Calculate as a minimum the below probabilities $a$ through $c$.  Assume the small letter "x" is estimated as the median of the $X$ variable, and the small letter "y" is estimated as the 1st quartile of the $Y$ variable.  Interpret the meaning of all probabilities.**  
 
```{r}
x <- median(X)
y <- as.numeric(quantile(Y)[2])
z <- (y - mu) / sd 

pXlx <- ((x-min(X))*(1/N))
pXgx <- 1 - ((x-min(X))*(1/N))
pXgy <- 1 - ((y-min(X)) * (1/N))
pYgy <- 1 - pnorm(z)
pYly <- pnorm(z)

``` 
  
**a. P(X>x | X>y)**  
Conditional probability - probability that $X$ is greater than $x$ given that $X$ is greater than $y$.

$$P(X>x | X>y) = \frac{P(X>x \cap X>y)}{P(X>y)} = \frac{P(X>x) P(X>y)}{P(X>y)}$$
```{r}

(pXgx*pXgy)/pXgy

```

**b. P(X>x, Y>y)**	
Joint Probability - Probability that $X$ is greater than $x$ and $Y$ is greater than $y$.
$$P(X>x, Y>y) = P(X>x \cap Y>y) = P(X>x) P(Y>y)$$
```{r}

pXgx*pYgy

```

**c. P(X<x | X>y)** 
Conditional probability - probability that $X$ is less than $x$ given that $X$ is greater than $y$. 
$$P(X<x | X>y) = \frac{P(X<x \cap X>y)}{P(X>y)} = \frac{P(X<x) P(X>y)}{P(X>y)}$$
```{r}

(pXlx*pXgy)/pXgy

```

### Probability (5 pts)  
**Investigate whether $P(X>x$ $and$ $Y>y) = P(X>x)P(Y>y)$ by building a table and evaluating the marginal and joint probabilities.**  

We will create a matrix of joint probabilities and then calculate the marginal probabilities. We will then compare the joint probability corresponding to record $[1,1]$ in the matrix to the product of the marginal probabilities (records $[3,1]$ and $[1,3]$ in the matrix).
```{r}

# calculate joint probabilities
a <- pXgx * pYgy
b <- pXgx * pYly
c <- pXlx * pYgy
d <- pXlx * pYly

# create matrix 
jointMatrix <- matrix(c(a,b,c,d), 
                  nrow=2)

# add row totals
fullMatrix <- rbind(jointMatrix, c(a+b, c+d))

# add column totals
fullMatrix <- cbind(fullMatrix, c(a+c, b+d, NA)) 

colnames(fullMatrix) <- c('P(X>x)', 'P(X<x)', 'Marginal Prob Y')
rownames(fullMatrix) <- c('P(Y>y)', 'P(Y<y)', 'Marginal Prob X')


fullMatrix

```

```{r}

margProb <- fullMatrix[3,1] * fullMatrix[1,3]
jointProb <- fullMatrix[1,1]

margProb 
jointProb

```

The two probabilities are equal.  
  
### Probability (5 pts)    
**Check to see if independence holds by using Fisherâ€™s Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?**  

Random variables are independent if neither variable affects the probability distribution of the other. Both Fisher's and Chi-Square examine the likelihood of independence of given variables. 
  
In both cases, we test the hypothesis that the variables are independent:

* $H_0$: The two variables are independent.
* $H_A$: The two variables are not independent.

A $p-value \leq 0.05$ allows us to reject $H_0$ and accept $H_A$. In this case, we can prove that the two variables are independent if the $p-value > 0.05$.

```{r warning=FALSE}

fisher.result <- fisher.test(jointMatrix)
print(fisher.result$p.value)
```
```{r warning=FALSE}
chi.result <- chisq.test(jointMatrix)
print(chi.result$p.value)

```

In both instances, the p-value is very high $(1)$, which means we accept $H_0$ - the variables are independent.  

The chi-square test can be used for any sized contingency table, whereas Fisher's exact test can only be used for $2x2$ contingency tables. Additionally, from reading online, it looks like Fisher's exact test is generally preferred for small sample sizes, whereas chi-square may be unreliable if the sample is too small.

In this case, I would recommend using the chi-square test because we have 2 random variables that both have $10,000$ numbers, to this test might be more accurate. 

# Problem 2
**You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.  https://www.kaggle.com/c/house-prices-advanced-regression-techniques **  

```{r}
trainSet <- read.csv('train.csv', sep=',') %>%
  as_tibble()

testSet <- read.csv('test.csv', sep=',') %>%
  as_tibble()

```
  
## Descriptive and Inferential Statistics (5 points)

**Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any three quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?**  
  
### Plot the data
We'll take a look at the sale prices:
```{r}

hist(trainSet$SalePrice,
     main="Distribution of Sale Prices",
     xlab="Sale Price",
     breaks = 20)

```

The majority of homes fall between $\$100,000$ and $\$200,000$.

``` {r}
boxplot(trainSet$SalePrice,
        main="Boxplot of Sale Prices",
        xlab="Sale Price")

```

The boxplot shows us that we have quite a few outliers. 

### Univariate Descriptive Statistics
```{r}

summary(trainSet)

```


### Scatterplot Matrix
* **Dependent Variable**: SalePrice
* **Independent Variables**: OverallQual, OverallCond

```{r}

spMatrix <- trainSet %>%
  select(SalePrice, OverallQual, OverallCond) 

pairs(spMatrix, gap=0.2)

```

**Quality** - We can see from the graph below that as the quality of the home increases, so does the sale price. 
  
**Condition** - There's a lot more variation in the sale price compared to the overall condition of the home (especially around condition = 5), but in general, we can see as the condition improves, the sale price increases. 

### Correlation Matrix
* **Quantitative Variables**: LotArea, TotalBsmtSF, GrLivArea
  
```{r}

matrixVals <- trainSet %>%
  select(LotArea, TotalBsmtSF, GrLivArea) 

corrMatrix <- cor(matrixVals)
corrMatrix

```
None of the variables appear to be that correlated -- the highest correlation is between GrLivArea and TotalBsmtSF.

### Pairwise correlations

For this analysis, we will assume the following:

* $H_0$: The true correlation between the variables is $0$.
* $H_A$: The true correlation between the variables is not $0$.

If the $p-value \leq 0.05$, we will reject $H_0$ and accept $H_A$. This would mean that there is a non-zero correlation between the variables. 

**Lot Area vs Total Basement Square footage**
```{r}

laBsf <- cor.test(matrixVals$LotArea, matrixVals$TotalBsmtSF, method = 'pearson',conf.level = 0.8)

laBsf$p.value

```

**Lot Area vs Living Area**
```{r}

laLa <- cor.test(matrixVals$LotArea, matrixVals$GrLivArea, method = 'pearson',conf.level = 0.8)

laLa$p.value

```


**Total Basement Square footage vs Living Area**
```{r}

BsfLa <- cor.test(matrixVals$TotalBsmtSF, matrixVals$GrLivArea, method = 'pearson',conf.level = 0.8)

BsfLa$p.value

```

In each pairwise-comparison, the $p-value <= 0.05$, which means that we can reject $H_0$ and accept the null. This means that the variables are not independent of each other - and regardless of how slight, they are all at least somewhat correlated to each other.  
  
Familywise error is the likelihood of accepting a false positive - in this case, reporting that there is a correlation between variables when in fact they have occurred by chance. Because the $p-values$ are so much lower than $0.05$, I would not be worried about familywise error.

### Linear Algebra and Correlation (5 points)  

**Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix. ** 

```{r}
precMatrix  <- inv(corrMatrix)
precMatrix
```

```{r}

cp <- corrMatrix %*% precMatrix
pc <- precMatrix %*% corrMatrix

cp
pc

```
  
The precision matrix * correlation matrix is about equal to the correlation matrix * precision matrix, which makes sense because they are inverses. 

LU Decomposition - I am shortcutting for this problem by using the lu.decomposition function from the matrixcalc library. To see a more detailed analysis of the LU decomposition function, please visit: https://rpubs.com/amberferger/DATA605_HW2_PS2 

```{r}
lu <- lu.decomposition(corrMatrix)
l <- lu$L
u <- lu$U

l
u
```

We can check that the LU Decomposition worked by comparing $l * u$ to the original matrix:

```{r}
l %*% u
corrMatrix

```
  
### Calculus-Based Probability & Statistics (5 points) 

**Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.  Then load the MASS package and run fitdistr to fit an exponential probability density function.  (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of $\lambda$ for this distribution, and then take $1,000$ samples from this exponential distribution using this value (e.g., $rexp(1,000, \lambda)$). Plot a histogram and compare it with a histogram of your original variable. Using the exponential pdf, find the $5^{th}$ and $95^{th}$ percentiles using the cumulative distribution function (CDF). Also generate a $95\%$ confidence interval from the empirical data, assuming normality. Finally, provide the empirical $5^{th}$ percentile and $95^{th}$ percentile of the data. Discuss.**  

I will use the variable **X1stFlrSF**.  
```{r}

hist(trainSet$X1stFlrSF)

```


**Documentation for function output:**  
An object of class "fitdistr", a list with four components,

* estimate - the parameter estimates
* sd - the estimated standard errors
* vcov - the estimated variance-covariance matrix
* loglik - the log-likelihood.
```{r}
exPDF <- fitdistr(trainSet$X1stFlrSF, "exponential")
lam <- as.numeric(exPDF$estimate)
samps <- rexp(1000,lam)

```
  
### Modeling (10 points) 
**Build some type of multiple regression  model and submit your model to the competition board. Provide your complete model summary and results with analysis. Report your Kaggle.com user name and score.**

